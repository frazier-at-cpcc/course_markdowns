# OpenHack – Modern Data Warehousing

**Product ID**: 23070
**Category ID**: nan
**Modality**: 1
**Active**: True
**Language**: en
**Product Code**: OHMDW
**Vendor Code**: MS
**Vendor Name**: Microsoft
**URL**: [Course Link](https://www.fastlaneus.com/course/microsoft-ohmdw)

## Objective
By the end of the OpenHack, attendees will have built out a technical solution that is a fully operating Modern Data Warehouse with corresponding CI/CD pipeline that takes into account data management – which meets top-quality data consumption requirements, like reliability, scalability, and maintainability.



- Modern cloud solution that results in higher reliability, scalability, and maintainability of large amounts of data.
- Introduction to new data storage services to meet unique and multiple data streams needs

## Essentials
Knowledge Prerequisites 
To be successful and get the most out of this OpenHack, participants should have existing knowledge of relational database structures and concepts (e.g. tables, joins, SQL) and experience with either SSIS or programing languages like Scala or Python. Previous experience creating ETL pipelines, source control management, automated testing, and build and release automation will help you advance more quickly.
Also, recommend familiarity with Azure fundamentals. 

Tooling Prerequisites
To avoid any delays with downloading or installing tooling, you are encouraged to have the following ready to go!



- Install your choice of Integrated Development Environment (IDE) Software, i.e. Visual Studio/ Visual Studio Code /Eclipse/IntelliJ
- Download Azure CLI
- SQL Server Database Tooling (Azure Data Studio/SSMS)
- SQL Server Data Tools (including BI tools) – If using Visual Studio for IDE
Post Learning Recommendations



- Implement a Data Warehouse with Azure SQL Data Warehouse
- Large-Scale Data Processing with Azure Data Lake Storage Gen2
- Core Cloud Services - Azure data storage options
- Azure for the Data Engineer
- Perform data engineering with Azure Databrick
- Architect a data platform in Azure

## Audience
- App Developers
- Customers that are trying to handle and store data from multiple sources
- Customers who need a DevOps solution that considers data management

## Outline
Challenge 1: Select and provision storage for an enterprise data lake
In this challenge, you will… 


Learning objectives:



- Compare and contrast Azure storage offerings
- Provision the selected Azure storage service
Challenge 2: Ingest data from cloud sources
In this challenge, you will….


Learning objectives:



- Orchestrate the ingestion of data from multiple cloud-based sources to a single cloud-based store
- Ensure the protection of specific customer data at all times leveraging the current technology set and solution architecture
Challenge 3: Pull data from on-premises and establish source control
In this challenge, you will… 


Learning objectives:



- Orchestrate the ingestion of data specifically from maintained “on-premises” solutions
- Implement a cloud-based source control repository for the developed solution
Challenge 4: Transform and normalize data within the lake and establish branch policies
In this challenge, you will… 


Learning objectives:



- Transform data into a normalized schema for downstream consumption
- Create new policies to make certain all future changes leverage an appropriate review process
Challenge 5: Populate a data warehouse and implement unit tests
In this challenge, you will… 


Learning objectives:



- Transform the data from the various source systems into a common data warehouse schema to support the generation of specific reports mandated by the business
- Orchestrate the dataflow into the data warehouse in an automated manner
- Build out unit tests across core components of the data pipeline
- Integrate automated testing into the code review process
Challenge 6: Differential data loads and telemetry
In this challenge, you will….


Learning objectives:



- Modify the solution to include doing differential data loads as well as the original bulk load
- Automate data load and processing to run daily
- Implement rich telemetry into the dataflow and deployment pipelines
- Add error handling to raise pipeline issues in real-time
Challenge 7: Automated deployment with validation and approval
In this challenge, you will… 


Learning objectives:



- Operationalize the solution deployment process through automation
- Create and implement a testing environment
- Implement automated deployment processes and policies

## Summary
Please note attendees work together in teams of 5 as a minimum and the pricing advertised is per team of 5.

## Course Duration
3 days

## Last Changed
2024-01-24T21:23:49.000Z

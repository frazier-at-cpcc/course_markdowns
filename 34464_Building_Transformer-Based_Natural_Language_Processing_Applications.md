# Building Transformer-Based Natural Language Processing Applications

**Product ID**: 34464
**Category ID**: nan
**Modality**: 1
**Active**: True
**Language**: en
**Product Code**: BNLPA
**Vendor Code**: NV
**Vendor Name**: Nvidia
**URL**: [Course Link](https://www.fastlaneus.com/course/nv-bnlpa)

## Objective
- How transformers are used as the basic building blocks of modern LLMs for NLP applications
- How self-supervision improves upon the transformer architecture in BERT, Megatron, and other LLM variants for superior NLP results
- How to leverage pretrained, modern LLM models to solve multiple NLP tasks such as text classification, named-entity recognition (NER), and question answering
- Leverage pre-trained, modern NLP models to solve multiple tasks such as text classification, NER, and question answering
- Manage inference challenges and deploy refined models for live applications

## Essentials
- Experience with Python coding and use of library functions and parameters
- Fundamental understanding of a deep learning framework such as TensorFlow, PyTorch, or Keras
- Basic understanding of neural networks

## Audience
nan

## Outline
Not available

## Summary
Learn how to apply and fine-tune a Transformer-based Deep Learning model to Natural Language Processing (NLP) tasks.

In this course, you'll:


- Construct a Transformer neural network in PyTorch
- Build a named-entity recognition (NER) application with BERT
- Deploy the NER application with ONNX and TensorRT to a Triton inference server
Upon completion, youâ€™ll be proficient in task-agnostic applications of Transformer-based models.

Please note that once a booking has been confirmed, it is non-refundable. This means that after you have confirmed your seat for an event, it cannot be cancelled and no refund will be issued, regardless of attendance.

## Course Duration
1 day

## Last Changed
2024-09-24T18:43:04.000Z
